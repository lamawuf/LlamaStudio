#!/usr/bin/env python3
"""
–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –°–ë–û–† ‚Äî —Å–∞–º –∫–ª–∏–∫–∞–µ—Ç, —Å–∞–º –ª–∏—Å—Ç–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
"""

import csv
import time
import re
from datetime import datetime
from playwright.sync_api import sync_playwright


def main():
    leads = []
    seen_phones = set()

    print(f"\n{'='*55}")
    print("  –ê–í–¢–û–ú–ê–¢ ‚Äî —Å–∞–º –∫–ª–∏–∫–∞–µ—Ç, —Å–∞–º –ª–∏—Å—Ç–∞–µ—Ç")
    print(f"{'='*55}\n")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, args=['--start-maximized'])
        page = browser.new_page(no_viewport=True)

        page.goto("https://2gis.ru/krasnodar/search/—Ä–µ–º–æ–Ω—Ç%20–∫–≤–∞—Ä—Ç–∏—Ä")
        print("üåê –ó–∞–≥—Ä—É–∂–∞—é 2GIS...\n")
        time.sleep(3)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–æ–≤–∏–∑–Ω–µ
        print("üìã –°—Ç–∞–≤–ª—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É '–ü–æ –Ω–æ–≤–∏–∑–Ω–µ'...")
        try:
            # –ö–ª–∏–∫ –Ω–∞ –≤—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            sort_dropdown = page.locator('xpath=//*[@id="root"]/div/div/div[1]/div[1]/div[3]/div/div/div[2]/div/div/div/div[2]/div[1]/div/div/div/div/div/div/div[2]/div/div/div/div/div/div/ul/li[1]/div/div[1]/div/span')
            sort_dropdown.click()
            time.sleep(1)
            # –ö–ª–∏–∫ –Ω–∞ "–ü–æ –Ω–æ–≤–∏–∑–Ω–µ"
            novizne_btn = page.locator('xpath=//*[@id="root"]/div/div/div[1]/div[1]/div[3]/div/div/div[2]/div/div/div/div[2]/div[1]/div/div/div/div/div/div/div[2]/div/div/div/div/div/div/ul/li[1]/div/div[2]/button[2]/span')
            novizne_btn.click()
            time.sleep(2)
            print("‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞\n")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É: {e}\n")

        # –ü–µ—Ä–µ–π–¥—ë–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É 21
        print("‚è© –ü–µ—Ä–µ—Ö–æ–∂—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É 21...")
        # –ö–ª–∏–∫–∞–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —á—Ç–æ–±—ã –¥–æ–±—Ä–∞—Ç—å—Å—è –¥–æ 21
        for skip_page in [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]:
            try:
                page.locator(f'text="{skip_page}"').first.click()
                time.sleep(1)
            except:
                pass

        current_page = 21
        max_pages = 999  # –î–æ –∫–æ–Ω—Ü–∞

        while current_page <= max_pages:
            print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page}/{max_pages}")

            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–æ–º–ø–∞–Ω–∏–π –≤ —Å–ø–∏—Å–∫–µ
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –Ω–∞ /firm/
            cards = page.locator('a[href*="/firm/"]').all()

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —á—Ç–æ –≤ —Å–ø–∏—Å–∫–µ —Å–ª–µ–≤–∞ (–Ω–µ –≤ —Ä–µ–∫–ª–∞–º–µ)
            firm_links = []
            for card in cards:
                try:
                    href = card.get_attribute('href')
                    if href and '/firm/' in href and 'tab' not in href:
                        firm_links.append(card)
                except:
                    pass

            print(f"   –ù–∞–π–¥–µ–Ω–æ {len(firm_links)} –∫–æ–º–ø–∞–Ω–∏–π")

            # –ö–ª–∏–∫–∞–µ–º –Ω–∞ –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É
            clicked = set()
            for i, card in enumerate(firm_links):
                try:
                    href = card.get_attribute('href')
                    if href in clicked:
                        continue
                    clicked.add(href)

                    card.click()
                    time.sleep(1.2)

                    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ
                    parse_company(page, leads, seen_phones)

                except Exception as e:
                    continue

            print(f"   –°–æ–±—Ä–∞–Ω–æ –ª–∏–¥–æ–≤: {len(leads)}\n")

            # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if current_page < max_pages:
                try:
                    next_btn = page.locator(f'text="{current_page + 1}"').first
                    if next_btn:
                        next_btn.click()
                        time.sleep(2)
                        current_page += 1
                    else:
                        # –ü—Ä–æ–±—É–µ–º –∫–Ω–æ–ø–∫—É ">"
                        next_arrow = page.locator('[aria-label="–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"], text="‚Ä∫"').first
                        if next_arrow:
                            next_arrow.click()
                            time.sleep(2)
                            current_page += 1
                        else:
                            break
                except:
                    break
            else:
                break

        browser.close()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    save_leads(leads)


def parse_company(page, leads, seen_phones):
    """–ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏"""

    try:
        # –¢–µ–ª–µ—Ñ–æ–Ω
        phone_els = page.query_selector_all('a[href^="tel:"]')
        for phone_el in phone_els:
            href = phone_el.get_attribute('href')
            if not href:
                continue

            phone = re.sub(r'[^\d+]', '', href.replace('tel:', ''))
            if phone.startswith('8') and len(phone) == 11:
                phone = '+7' + phone[1:]

            if len(phone) < 11 or phone in seen_phones:
                continue

            seen_phones.add(phone)

            # –ù–∞–∑–≤–∞–Ω–∏–µ
            name = "–ö–æ–º–ø–∞–Ω–∏—è"
            try:
                h1 = page.query_selector('h1')
                if h1:
                    name = h1.inner_text().strip()
            except:
                pass

            # URL
            current_url = page.url

            # –°–æ—Ü—Å–µ—Ç–∏
            SOCIALS = {
                'vk.com': 'VK', 'vk.ru': 'VK',
                't.me': 'Telegram', 'telegram': 'Telegram',
                'wa.me': 'WhatsApp', 'whatsapp': 'WhatsApp',
                'instagram': 'Instagram', 'youtube': 'YouTube',
                'ok.ru': 'OK', 'taplink': 'Taplink', 'linktree': 'Linktree',
            }
            NOT_A_SITE = list(SOCIALS.keys()) + ['2gis', 'facebook', 'fb.com', 'tiktok', 'rutube', 'viber']

            has_real_site = False
            found_socials = []

            links = page.query_selector_all('a')
            for link in links:
                text = (link.inner_text() or '').strip().lower()
                href_link = (link.get_attribute('href') or '').lower()

                for key, label in SOCIALS.items():
                    if key in href_link or key in text:
                        if label not in found_socials:
                            found_socials.append(label)

                if re.match(r'^[a-z–∞-—è—ë0-9][a-z–∞-—è—ë0-9\-\.]*\.(ru|com|—Ä—Ñ|pro|su|net|org|biz|info|online|site|store|shop)$', text):
                    if not any(s in text for s in NOT_A_SITE):
                        has_real_site = True
                        break

            if has_real_site:
                print(f"   ‚è≠ {name[:35]} ‚Äî –µ—Å—Ç—å —Å–∞–π—Ç")
                return

            social_str = ', '.join(found_socials) if found_socials else ''
            leads.append({
                'name': name,
                'phone': phone,
                'social': social_str,
                '2gis_url': current_url
            })

            social_info = f" [{social_str}]" if social_str else ""
            print(f"   ‚úÖ {name[:35]}{social_info}")
            print(f"      üìû {phone}")
            return

    except:
        pass


def save_leads(leads):
    if leads:
        filename = f"leads_auto_{datetime.now().strftime('%H%M')}.csv"
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'phone', 'social', '2gis_url'])
            writer.writeheader()
            writer.writerows(leads)

        print(f"\n{'='*55}")
        print(f"  ‚úÖ –°–û–ë–†–ê–ù–û: {len(leads)} –ª–∏–¥–æ–≤")
        print(f"  üìÅ –§–∞–π–ª: {filename}")
        print(f"{'='*55}\n")
    else:
        print("\n‚ùå –ù–∏—á–µ–≥–æ –Ω–µ —Å–æ–±—Ä–∞–Ω–æ")


if __name__ == '__main__':
    main()
